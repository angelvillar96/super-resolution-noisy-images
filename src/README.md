# Code Structure

This README describes the functionality of the different scripts included in the project.

All classes and methods in the project have their corresponding *docstring* explaining their functionality, input arguments and returned values. I refer to the *docstring* for particular documentation of each method.


* [1. Necessary Scripts](#necessary-scripts)
* [2. Metrics and Comparison Scripts](#metrics-and-comparison-scripts)
* [3. Auxiliary Scripts](#auxiliary-scripts)
* [4. Hints](#hints)



## Necessary Scripts

The files explained in this section correspond to the ones whose execution is necessary for some step of the super-resolution and denoising pipeline.

 - **config.py**: Determines the paths where experiments, datasets and evaluations will be saved. Furthermore, it defines the seed for the random number generator.

 - **01_create_experiment.py**: Creates an experiment folder under the */experiments* directory. The experiment parameters are given as command line arguments and then saved in the experiment directory as a JSON file.

 - **02_train_denoiser_autoencoder.py**: If the denoising method selected in the experiment is *denoiser_autoencoder*, this script must be run prior to training the SR model. This file instanciates a denoising autoencoder and trains it to denoise low-resolution images. The trained model is then saved in the */models/autoencoder* folder in the experiment directory.

 - **02_train_model.py**: Trains the SR model, joined with the corresponding denoiser, for the number of epochs specified in the
*experiment_parameters* JSON file. Furthermore, the train and validation loss and metrics will be saved (each epoch) to the *training_logs.json* file, checkpoint models will be saved every 10 epochs, and a figure assessing the quality of reconstruction will be created in the */plots/valid_plots* directory every 5 epochs.

- **03_evaluate_model.py**: Loads a model checkpoint given the epoch number and evaluates its performance on the test set.

- **03_test_accross_epochs.py**: Iteratively loads the model checkpoints and evaluates them on the test set. The test loss and metrics are stored in the *training_logs.json*. This file iteratively calls *03_evaluate_model.py*.

- **04_deploy_pretrained**: Loads the checkpoint with the best validation loss and performs denoising and super-resolution for a few test-set images, which are then saved in the */plots/test_plots* directory. This file illustrates how to load one of our pretrained models to use it in practice.


## Metrics and Comparison Scripts

In this section, we give an overview of the files that are used to generate metric plots and to compare different experiments.

 - **04_generate_metric_plots.py**: Uses the values stored in the *training_logs* JSON file to generate plots displaying the MSE, MAE and PSNR landscapes for training, validation and test epochs, using linear and logarithmic axis.

 - **04_generate_autoencoder_metric_plots.py**: Generates plots displaying the training and validation loss of a denoising autoencoder.

 - **04_training_landscapes_doku.py**: Small modification of the *04_generate_metric_plots.py*. This file was simply used to generate some plots for the report.

 - **05_create_comparison_evaluation_data.py**: This file loads the results from several experiments (specified as arguments) and unifies them into a JSON file, which is saved in the */eval_results/eval_checkpoint_dicts* directory.

 - **05_generate_comparison_metric_plots.py**: Generates metric plots, similarly to *04_generate_metric_plots.py*, but jointly displaying the results for experiments using different denoisers. The following figure displays an example of a plot generated using this script.

  ![comparison metric plot](https://github.com/angelvillar96/denoising_in_superresolution/blob/denoising/eval_results/eval_metric_plots/sequence_no_denoiser_eval--sequence_exps_eval--sequence_exps_autoencoder_mnist_eval_psnr.png "Comparison Metric Plot")

 - **05_generate_hyper_parameter_metric_comparison.py**: Similar to *05_generate_comparison_metric_plots.py*, but displays the results for experiments comparing the role of hyper-parameters.

 - **06_create_image_comparison.py**: Generates figures comparing the reconstructions generated by models using different denoisers and architectures.  The following image displays an example of a figure generated using this script.

 ![image comparison](https://github.com/angelvillar96/denoising_in_superresolution/blob/denoising/eval_results/eval_image_comparison/comparison_svhn_speckle_0.4_4.png "Image Comparison")

 - **06_create_hyper_parameter_comparison.py**: Similar to *06_create_image_comparison.py*, but compares reconstructions generated by models using the same denoiser but with different hyper-parameters.

## Auxiliary Scripts

The files here mentioned are the ones whose execution is neither necessary for the super-resolution pipeline nor for evaluations or comparisons. These scripts belong to two categories:

  **1.** The files starting with the prefix **aux_** correspond to Sanity Checks used to make sure that the experiment parameters have been set correctly. These scripts perform various tasks such as displaying a small subset of the training data or exporting the network architecture to a *.txt* file.

  **2.** The files with the prefix **sequence_** correspond to bash scripts used to perform some task several times for different sets of input parameters. These scripts sequentially call the [Necessary Scripts](#necessary-scripts) to, e.g., train models with different parameters or evaluate a model using  differnet corruption schemes.

## Hints

   - Run `METHOD.__doc__` or `help(METHOD)` to display the documentation of a method or class.

 ```python
 >>> print(noisers.Noiser.__call__.__doc__)

         Method that adds the noise to the original image

         Args:
         -----
         img: numpy array
             original image to which the noise will be added

         Returns:
         --------
         corrupted_img: numpy array
             Image to which the noise has been added.
             It has the same shape as the input image
 ```

- Run `python FILE_NAME.py --help` to display the command line arguments of the given script.

  ```shell
    $ python 03_evaluate_model.py --help

       usage: 03_evaluate_model.py [-h] [-d EXP_DIRECTORY] [-e EPOCH]
                                   [--noise NOISE]  [--std STD]

       optional arguments:
         -h, --help            
               show this help message and exit
         -d EXP_DIRECTORY, --exp_directory EXP_DIRECTORY
               Path to the experiment directory
         -e EPOCH, --epoch EPOCH
               Epoch of the checkpoint to process
         --noise NOISE         
               Type of noise to corrupt the test-set images ['', 'gaussian', 'poisson', 'speckle', 'salt_pepper']
         --std STD
               Standard deviation of the noise
 ```
